{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Продвинутый блок | Введение в обучение с подкреплением, алгоритм Q-обучение | Ультра-лайт задание | УИИ.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNYfG1JdhOyk3+eGSlAlTXF"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"C-evc9218s67"},"source":["В этом задании, вам необходимо воспроизвести программу, которая обучает Q-таблицу для нахождения кротчайщего пути. Рутинный код, который отображает графики, можно скопировать с лекционного ноутбука - но весь остальной код реккомендуется писать самому.\n","\n","Далее, нужно провести несколько простых экспериментов с параметрами `epsilon` и `kf_rewards`.\n","\n","* В первом эксперименте, укажите значения `epsilon = 1` и `kf_rewards = 0.99`.\n","\n","* Во втором эксперименте, укажите значения `epsilon = 1` и `kf_rewards = 1`.\n","\n","* В третьем эксперименте, укажите значение `epsilon = 0` и `kf_rewards = 0.99`.\n","\n","* В четвертом эксперименте, укажите значение `epsilon = 1` и `kf_rewards = 0`.\n","\n","Имейте введу, что значение `epsilon` будет постепенно падать в ходе обучение алгоритма.\n","\n","После того, как вы проведёте все 4 эксперимента, напишите свои выводы касаемо того, как агент обучается при каждом наборе параметров."]},{"cell_type":"code","metadata":{"id":"9ITfS1al-lj4"},"source":["#Импорт библиотек\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import matplotlib.patches as patches\n","import seaborn as sns\n","from IPython.display import clear_output\n","import time\n","sns.set_style('darkgrid')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fHB0ubDJ-heq"},"source":["# Ваш код"],"execution_count":null,"outputs":[]}]}